import streamlit as st
from google import genai
import time
import pandas as pd
import plotly.express as px
from groq import Groq

st.set_page_config(page_title="LLM Benchmarking", layout="wide")

st.title("LLM Benchmarking Dashboard")
st.subheader("Compare as many LLMs as you want side by side")
st.divider()

client = genai.Client(api_key=st.secrets["GEMINI_API_KEY"])
groq_client = Groq(api_key=st.secrets["GROQ_API_KEY"])

def call_gemini(prompt):
    start_time = time.time()
    response = client.models.generate_content(model="gemini-2.5-flash", contents=prompt)
    end_time = time.time()

    if response.usage_metadata:
        token_count = response.usage_metadata.total_token_count
    else:
        token_count = len(response.text) // 4

    return response.text, end_time - start_time, token_count

def call_llama(prompt):
    start_time = time.time()
    response_groq = groq_client.chat.completions.create(
        model="llama-3.1-8b-instant",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.5
    )
    end_time = time.time()

    content = response_groq.choices[0].message.content
    token_count = response_groq.usage.total_tokens
    
    return content, end_time - start_time, token_count

with st.sidebar:
    st.title("Choose models")
    use_gemini = st.checkbox("Gemini 2.5 Flash", value=True)
    use_groq = st.checkbox("Llama 3.1 8B Instant", value=True)

prompt = st.chat_input("Enter your prompt")

if prompt:
    comparisions = []
    
    if use_gemini:
        comparisions.append("Gemini 2.5 Flash")
    
    if use_groq:
        comparisions.append("Llama 3.1 8B Instant")

    cols = st.columns(len(comparisions))
    results = []

    for i, comparision_name in enumerate(comparisions):
        with cols[i]:
            st.subheader(comparision_name)
            if comparision_name == "Gemini 2.5 Flash":
                content, latency, token_count = call_gemini(prompt)
            elif comparision_name == "Llama 3.1 8B Instant":
                content, latency, token_count = call_llama(prompt)

            # Display latency and token count
            st.caption(f"Latency: {latency:.2f} seconds | Tokens: {token_count}")
            # Display the model's response
            st.write(content)
        
            if latency > 0:
                results.append({
                    "Model": comparision_name,
                    "Latency (s)": latency,
                    "Tokens": token_count,
                    "Throughput (tokens/s)": token_count / latency
                })  
    if results:
        df = pd.DataFrame(results)
        st.divider()
        st.subheader("Performance Comparison")
        st.dataframe(df)

        fig = px.bar(df, x="Model", y="Throughput (tokens/s)", title="Throughput Comparison", text="Throughput (tokens/s)")
        st.plotly_chart(fig, use_container_width=True)